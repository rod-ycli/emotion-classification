{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c09547e-9a7d-4fb5-94ca-1e87df66694b",
   "metadata": {},
   "source": [
    "# 2. Tweets, BoW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edd1c61-f66c-460b-931d-6e807aa337bc",
   "metadata": {},
   "source": [
    "## 2.1 Data preparation and filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e247abb7-7266-4851-8a4a-44d1c3c5bf6d",
   "metadata": {},
   "source": [
    "(a) Loading the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c7a0c11-8289-452b-a315-ccf06e14cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filepath = 'data/wassa/training/all.train.tsv'\n",
    "tweets_dftrain = pd.read_csv(filepath, sep='\\t')\n",
    "\n",
    "filepath = 'data/wassa/testing/all.test.tsv'\n",
    "tweets_dftest = pd.read_csv(filepath, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba49719-fffb-48d7-a8df-7f28acbee7b0",
   "metadata": {},
   "source": [
    "(b) Tokenizing and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe6eea66-fc87-450c-96d6-afc44f8004fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Using spaCy to tokenize the sentences\n",
    "training_data_2 = [nlp(sent) for sent in list(tweets_dftrain['Tweet'])]\n",
    "training_labels_2 = list(tweets_dftrain['Label'])\n",
    "\n",
    "test_data_2 = [nlp(sent) for sent in list(tweets_dftest['Tweet'])]\n",
    "test_labels_2 = list(tweets_dftest['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da34b922-7685-48e7-b745-17a88cc17995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sent in training_data_1:\n",
    "#     for token in sent:\n",
    "#         if \"I'm\" == token.text:\n",
    "#             print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b91bab-11ae-4540-8542-43c01b7d6d32",
   "metadata": {},
   "source": [
    "    (i) Filter A: MaxDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f367242c-6231-411a-993d-2ec6bbd9f456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min_df 2\n",
      "Max_df 361\n",
      "Rare words with low df =  5295 words. Examples:  ['athlete', 'rescue', 'part2', '81', 'victory.\\\\n\\\\ntomiho', 'Ô§ó', 'cowardliness', 'dnt', 'vest', '@pottzgame', 'raaar', '@mannequinpussy', 'lasting', 'yh', 'gameface', 'tracey', '@a_rockasthe', 'bbfail', 'mindset', '//rip//']\n",
      "Stop words with high df: {'the', ',', 'that', 'for', 'my', 'it', 'of', 'and', 'be', 'in', 'i', '#', 'on', '!', 'a', 'have', '.', 'you', ' ', 'do', 'to', \"n't\"}\n",
      "Size of the rest vocab: 4190\n",
      "Samples: [['m', 'so', 'mad', 'about', 'power', 'ranger', 'm', 'incense', 'm', 'furious'], ['wo', 'nt', 'use', 'use', '@mothercareuk', '@mothercarehelp', 'again', 'these', 'guy', 'ca', 'nt', 'get', 'nothing', 'right', 'fume'], ['bitch', 'aggravate', 'like', 'what', 'inspire', 'big', 'cunt', 'know', 'man', 'kind', '?'], ['why', '@dapperlaugh', 'come', 'glasgow', 'night', 'work', 'fucking', 'gutte', 'wait', 'an', 'appearance', 'age', 'rage'], ['fume', 'üò§'], ['zero', 'help', 'from', '@up', 'customer', 'service', 'just', 'push', 'buck', 'back', 'forth', 'promise', 'callback', 'n‚Äôt', 'happen', 'anger'], ['not', 'mention', 'guy', 'stop', 'but', 'let', \"'s\", '2', 'ppl', 'front', 'go', 'wtf', 'blood', 'boil'], ['hate', 'if', 'soul', \"'d\", 'fiery', 'hell'], ['why', 'people', 'so', 'offend', 'by', 'kendall', 'he', 'end', 'photo', 'shoot', 'like', 'seriously', 'shut', 'fuck', 'up'], ['about', 'block', 'everyone', 'everywhere', 'post', 'about', 'storm', 'think', 'everyone', 'aware', 'damn', 'rain', 'what', 'not', 'so', 'quit', 'damn']]\n"
     ]
    }
   ],
   "source": [
    "from utils import low_high_mid_df\n",
    "\n",
    "min_df = 2\n",
    "max_df = len(training_data_2)//10\n",
    "\n",
    "low_df, high_df, clean2A = low_high_mid_df(min_df, max_df, training_data_2)\n",
    "\n",
    "print(\"Rare words with low df = \", len(low_df), \"words. Examples: \", list(low_df)[:20])\n",
    "print(\"Stop words with high df:\", high_df)\n",
    "vocab_2A = set()\n",
    "for sent in clean2A:\n",
    "    for t in sent:\n",
    "        vocab_2A.add(t)\n",
    "print(\"Size of the rest vocab:\", len(vocab_2A))\n",
    "print(\"Samples:\", clean2A[10:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a43f3b-82f0-4a4b-8808-ce355f558703",
   "metadata": {},
   "source": [
    "    (ii) Filter B: DTandPRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e8e347d-a24f-47e5-bcf3-bdf3577a7cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determiner and pronouns {'no', 'this', '_', 'our', '@relaqss', '\\\\n\\\\nsam', '‚ù§', 'herself', '#', 'strength.\\\\nthey', 'üêà', 'they', 'myself', '@rowillfindyou', 'em', '@melissajoyrd', 'üò°', 'tbh', 'every', 'memphis', \"'s\", '‚Äôs', '@snub23', 'each', 'its', 'don‚Äôt', \"y'\", '@blackeyed_susie', 'üëÖ', '@ryyyshh', '@m_t_f_72', 'hbu', 'hers', 'üòß', 'he', 'a', 'both', '@neyaphemmaster', 'these', 'yours', 'üçÅ', 'her', ':)', 'you', 'himself', 'either', 'we', '@reyesaverie', '@missmeliss465', 'that', '\\\\nit', 'mine', '\\uf62b', 'i', 'one', '@fra93_bruno', 'scarred,\\\\nthis', 'thy', 'those', 'jut', 'ya', 'an', '@weebtard', 'y', 'another', '\\\\n\\\\nother', '‚ú®', 'oldham\\\\nnext', 'some', 'near,\\\\nthe', 'ty', 'itself', 'the', 'üêÆ', 'themselves', 'his', 'my', 'isthereahelplineforthis', 'it', 'd', '@british_airways', \"you're\", '@ntfc', 'xx', '@its.finfin', 'ek', 'your', 'she', 'u', 'eagles.\\\\nthey', '@kevincanwaitcbs', 'n', 'also-', 'yourself', '@adsbyflaherty', '@themathofyou', 'any', 'their', 'all', 'bridgetjonesbaby', '@mhchat', 'üòä', '@sargon_of_akkad', '@smshow', 'tho', 'ourselves', '\\\\nimagine', '\\\\n#you'}\n",
      "Min_df 2\n",
      "Rare words with low df =  5272 words. Examples: ['athlete', 'rescue', 'part2', '81', 'victory.\\\\n\\\\ntomiho', 'Ô§ó', 'cowardliness', 'dnt', 'vest', '@pottzgame', 'raaar', '@mannequinpussy', 'lasting', 'yh', 'gameface', 'tracey', '@a_rockasthe', 'bbfail', 'mindset', '//rip//']\n",
      "Size of the rest vocab: 4167\n",
      "Samples: [['m', 'so', 'mad', 'about', 'power', 'ranger', '.', 'm', 'incense', '.', 'm', 'furious', '.'], ['wo', 'nt', 'use', 'use', '@mothercareuk', '@mothercarehelp', 'again', '!', '!', 'guy', 'ca', 'nt', 'get', 'nothing', 'right', '!', '!', '#', 'fume'], ['bitch', 'aggravate', 'like', 'what', 'inspire', 'to', 'be', 'big', 'cunt', 'know', 'to', 'man', 'kind', '?'], ['why', 'do', '@dapperlaugh', 'have', 'to', 'come', 'to', 'glasgow', 'on', 'night', 'be', 'work', '.', 'be', 'fucking', 'gutte', ',', 'be', 'wait', 'for', 'appearance', 'for', 'age', '#', 'rage'], ['fume', 'üò§'], ['zero', 'help', 'from', '@up', 'customer', 'service', '.', 'just', 'push', 'buck', 'back', 'and', 'forth', 'and', 'promise', 'callback', 'that', 'do', 'n‚Äôt', 'happen', '.', '#', 'anger', '#'], ['not', 'to', 'mention', 'guy', 'stop', 'but', 'let', '2', 'ppl', 'in', 'front', 'of', 'go', '.', 'wtf', '.', 'blood', 'be', 'boil', '.'], ['hate', '.', 'if', 'have', 'soul', ',', \"'d\", 'to', 'fiery', 'of', 'hell', '.'], ['why', 'be', 'people', 'so', 'offend', 'by', 'kendall', 'end', 'photo', 'shoot', 'like', 'seriously', 'shut', 'fuck', 'up'], ['be', 'about', 'to', 'block', 'everyone', 'everywhere', 'post', 'about', 'storm', '.', 'think', 'everyone', 'be', 'aware', 'of', 'damn', 'rain', 'and', 'what', 'not', 'so', 'quit', '.', '#', 'damn']]\n"
     ]
    }
   ],
   "source": [
    "from utils import remove_DT_PRP\n",
    "\n",
    "min_df = 2\n",
    "\n",
    "low_df, DTandPRP_tok, clean2B = remove_DT_PRP(min_df, training_data_2)\n",
    "\n",
    "print(\"Rare words with low df = \", len(low_df), \"words. Examples:\", list(low_df)[:20])\n",
    "vocab_2B = set()\n",
    "for sent in clean2B:\n",
    "    for t in sent:\n",
    "        vocab_2B.add(t)\n",
    "print(\"Size of the rest vocab:\", len(vocab_2B))\n",
    "print(\"Samples:\", clean2B[10:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5503dcd-051f-456c-8c06-242dc3936abb",
   "metadata": {},
   "source": [
    "## 2.2 BoW vectorization and training the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98847da8-1120-4517-8908-39c1944c68b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f462c6-2441-453b-ba1d-ab4c3625dd4e",
   "metadata": {},
   "source": [
    "(a) Encoding training labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48136334-9fc4-46d2-b1b4-be32e57d605f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'fear', 'joy', 'sadness']\n"
     ]
    }
   ],
   "source": [
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit(training_labels_2+test_labels_2)\n",
    "print(list(label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62779ff8-6835-416f-8838-108b83cf6dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0]\n",
      "['anger', 'anger', 'anger', 'anger', 'anger']\n",
      "['How the fu*k! Who the heck! moved my fridge!... should I knock the landlord door. #angry #mad ##', \"So my Indian Uber driver just called someone the N word. If I wasn't in a moving vehicle I'd have jumped out #disgusted \", '@DPD_UK I asked for my parcel to be delivered to a pick up store not my address #fuming #poorcustomerservice', 'so ef whichever butt wipe pulled the fire alarm in davis bc I was sound asleep #pissed #angry #upset #tired #sad #tired #hangry ######', \"Don't join @BTCare they put the phone down on you, talk over you and are rude. Taking money out of my acc willynilly! #fuming\"]\n"
     ]
    }
   ],
   "source": [
    "training_classes = label_encoder.transform(training_labels_2)\n",
    "print(training_classes[:5])\n",
    "print(list(tweets_dftrain['Label'])[:5])\n",
    "print(list(tweets_dftrain['Tweet'])[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a55ac9e-a8d7-4c54-89cb-4210170a94cb",
   "metadata": {},
   "source": [
    "### Filter A: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448d503f-ba83-4f94-9f89-d7d47f061500",
   "metadata": {},
   "source": [
    "(a) Vectorise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47f37672-b95f-4b50-ad7b-b24fc8ec1f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A CountVectorizer which takes tokenized lists as input\n",
    "### Taken from https://stackoverflow.com/questions/35867484/pass-tokens-to-countvectorizer,\n",
    "### https://stackoverflow.com/questions/27673527/how-should-i-vectorize-the-following-list-of-lists-with-scikit-learn, 26 Oct 2021\n",
    "\n",
    "def dummy(x):\n",
    "    return x\n",
    "\n",
    "utterance_vec_2A = CountVectorizer(tokenizer=dummy, lowercase=False)\n",
    "\n",
    "training_count_vectors_2A = utterance_vec_2A.fit_transform(clean2A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc898b49-b088-4d36-bf33-4e76cb2711d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(training_count_vectors_2A .toarray()[2][200:280])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48c15e0d-73ce-49b7-94b9-1ad6e5180fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4190\n"
     ]
    }
   ],
   "source": [
    "#Total number of word features or the length of the total vector\n",
    "print(len(utterance_vec_2A.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "216a60ec-b42f-4ab2-9489-14cba72e689f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['  ', '   ', '\"', '#funny', '#whatever', '$', '%', '&', \"'\", \"'\\\\n\\\\nhe\", \"'d\", \"'everywhere\", \"'i\", \"'ll\", \"'s\", \"'ve\", '(', ')', '):', '*', '+', '-', '--', '-2.5', '-dalai', '-terrible-', '..', '...', '....', '.....', '..........', '.@divamagazine', '.@simonnricketts', '.@tolumanda', '/', '0', '1', '1/2', '10', '10/11', '100', '1000', '100k', '101', '10golds24', '12', '13', '130', '148', '15']\n"
     ]
    }
   ],
   "source": [
    "# First 50 feature names\n",
    "print(list(utterance_vec_2A.get_feature_names())[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9f5e12c-8c97-4596-8b32-b9bd7cbd1b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert raw frequency counts into TF-IDF values\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "training_tfidf_2A = tfidf_transformer.fit_transform(training_count_vectors_2A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f8b88474-444a-492c-9f4c-da5f4379221d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.3738906 0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.       ]\n"
     ]
    }
   ],
   "source": [
    "print(training_tfidf_2A.toarray()[2][200:280])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "96903b2b-b969-4e1a-9ed1-03507616e648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CalibratedClassifierCV(base_estimator=LinearSVC(max_iter=2000), cv=10)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "linear_model = svm.LinearSVC(max_iter=2000)\n",
    "svm_linear_clf_2A = CalibratedClassifierCV(linear_model , method='sigmoid', cv=10)\n",
    "\n",
    "svm_linear_clf_2A.fit(training_tfidf_2A, training_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a142e14-9ebe-4d66-944e-addc4eafbfeb",
   "metadata": {},
   "source": [
    "### Filter B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cd335d-3faa-47bf-85d9-7f8e00df61e9",
   "metadata": {},
   "source": [
    "(a) Vectorise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b77f2d46-1103-409f-9ebe-00368c7b589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance_vec_2B = CountVectorizer(tokenizer=dummy, lowercase=False)\n",
    "\n",
    "training_count_vectors_2B = utterance_vec_2B.fit_transform(clean2B)\n",
    "training_tfidf_2B = tfidf_transformer.fit_transform(training_count_vectors_2B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7f2f7511-9811-4f96-9014-6058acca3a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4167\n"
     ]
    }
   ],
   "source": [
    "#Total number of word features or the length of the total vector\n",
    "print(len(utterance_vec_2B.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf812ed1-8047-47bc-92c7-735adcbec91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '  ', '   ', '!', '\"', '#', '#funny', '#whatever', '$', '%', '&', \"'\", \"'\\\\n\\\\nhe\", \"'d\", \"'everywhere\", \"'i\", \"'ll\", \"'s\", \"'ve\", '(', ')', '):', '*', '+', ',', '-', '--', '-2.5', '-dalai', '-terrible-', '.', '..', '...', '....', '.....', '..........', '.@divamagazine', '.@simonnricketts', '.@tolumanda', '/', '0', '1', '1/2', '10', '10/11', '100', '1000', '100k', '101', '10golds24']\n"
     ]
    }
   ],
   "source": [
    "# First 50 feature names\n",
    "print(list(utterance_vec_2B.get_feature_names())[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001ea6e4-7962-4024-9a90-a0faeed5e730",
   "metadata": {},
   "source": [
    "(b) Train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bd6d36be-6001-4d01-a8a3-2864c686dece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CalibratedClassifierCV(base_estimator=LinearSVC(max_iter=2000), cv=10)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model = svm.LinearSVC(max_iter=2000)\n",
    "svm_linear_clf_2B = CalibratedClassifierCV(linear_model , method='sigmoid', cv=10)\n",
    "\n",
    "svm_linear_clf_2B.fit(training_tfidf_2B, training_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1bd11e-6826-4dbe-aedc-41c43bd7d6e1",
   "metadata": {},
   "source": [
    "## 2.3 Predicting the test data and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "baaa8b7e-e62b-429a-9991-c2afec624f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2646a9e2-5568-4801-934d-2582f38ea306",
   "metadata": {},
   "source": [
    "Encode the test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ccc6f43a-505d-4ccb-a298-42fc21361648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "['anger', 'anger', 'anger', 'anger', 'anger']\n",
      "['At the point today where if someone says something remotely kind to me, a waterfall will burst out of my eyes', \"@CorningFootball  IT'S GAME DAY!!!!      T MINUS 14:30  #relentless\", 'This game has pissed me off more than any other game this year. My blood is boiling! Time to turn it off! #STLCards', \"@spamvicious I've just found out it's Candice and not Candace. She can pout all she likes for me üòç\", \"@moocowward @mrsajhargreaves @Melly77 @GaryBarlow if he can't come to my Mum'a 60th after 25k tweets then why should I üôà  #soreloser\"]\n"
     ]
    }
   ],
   "source": [
    "test_classes = label_encoder.transform(test_labels_2)\n",
    "print(test_classes[:20])\n",
    "print(list(tweets_dftest['Label'])[:5])\n",
    "print(list(tweets_dftest['Tweet'])[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a547d4-e641-4de4-a4de-b9e792c339e2",
   "metadata": {},
   "source": [
    "### Filter A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6c2325e2-cbb8-4203-a8d1-82170e549670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min_df 2\n",
      "Max_df 314\n"
     ]
    }
   ],
   "source": [
    "max_df_test = len(test_data_2)//10\n",
    "\n",
    "low_df_test_2A, high_df_test_2A, test_mid_df_2A = \\\n",
    "low_high_mid_df(2, max_df_test, test_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5f325163-4e4f-4d40-93c8-842b5670ca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_count_2A = utterance_vec_2A.transform(test_mid_df_2A)\n",
    "test_tfidf_2A = tfidf_transformer.fit_transform(test_count_2A)\n",
    "\n",
    "y_pred_svm_2A = svm_linear_clf_2A.predict(test_tfidf_2A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "48c3259d-d9e0-491b-b758-718c4a8b9a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger' 'fear' 'joy' 'sadness']\n",
      "BoW TFIDF SVM LINEAR: Tweets, Filter A\n",
      "Word mininum document frequency 2 ; max: 314\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.826265  0.794737  0.810195       760\n",
      "           1   0.810811  0.783920  0.797138       995\n",
      "           2   0.835598  0.861345  0.848276       714\n",
      "           3   0.761571  0.806835  0.783550       673\n",
      "\n",
      "    accuracy                       0.809039      3142\n",
      "   macro avg   0.808561  0.811709  0.809790      3142\n",
      "weighted avg   0.809635  0.809039  0.809007      3142\n",
      "\n",
      "Confusion matrix SVM, BoW Tweets, Filter A\n",
      "['anger' 'fear' 'joy' 'sadness']\n",
      "[[604  68  34  54]\n",
      " [ 60 780  61  94]\n",
      " [ 21  56 615  22]\n",
      " [ 46  58  26 543]]\n"
     ]
    }
   ],
   "source": [
    "report2A = classification_report(test_classes,y_pred_svm_2A,digits = 4)\n",
    "print(label_encoder.classes_)\n",
    "print('BoW TFIDF SVM LINEAR: Tweets, Filter A')\n",
    "print('Word mininum document frequency', min_df, \"; max:\", max_df_test)\n",
    "print(report2A)\n",
    "\n",
    "print('Confusion matrix SVM, BoW Tweets, Filter A')\n",
    "print(label_encoder.classes_)\n",
    "print(sklearn.metrics.confusion_matrix(test_classes,y_pred_svm_2A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4d6dfac1-fa22-4750-821b-2828107a92dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anger</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>Chat</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Gold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>82.695987</td>\n",
       "      <td>15.140310</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>1.739462</td>\n",
       "      <td>At the point today where if someone says somet...</td>\n",
       "      <td>anger</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96.369466</td>\n",
       "      <td>0.461471</td>\n",
       "      <td>0.592322</td>\n",
       "      <td>2.576741</td>\n",
       "      <td>@CorningFootball  IT'S GAME DAY!!!!      T MIN...</td>\n",
       "      <td>anger</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.337530</td>\n",
       "      <td>3.101388</td>\n",
       "      <td>0.394256</td>\n",
       "      <td>8.166827</td>\n",
       "      <td>This game has pissed me off more than any othe...</td>\n",
       "      <td>anger</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67.636159</td>\n",
       "      <td>1.859540</td>\n",
       "      <td>3.174199</td>\n",
       "      <td>27.330102</td>\n",
       "      <td>@spamvicious I've just found out it's Candice ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44.852154</td>\n",
       "      <td>47.804198</td>\n",
       "      <td>1.132859</td>\n",
       "      <td>6.210789</td>\n",
       "      <td>@moocowward @mrsajhargreaves @Melly77 @GaryBar...</td>\n",
       "      <td>fear</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       anger       fear       joy    sadness  \\\n",
       "0  82.695987  15.140310  0.424242   1.739462   \n",
       "1  96.369466   0.461471  0.592322   2.576741   \n",
       "2  88.337530   3.101388  0.394256   8.166827   \n",
       "3  67.636159   1.859540  3.174199  27.330102   \n",
       "4  44.852154  47.804198  1.132859   6.210789   \n",
       "\n",
       "                                                Chat Prediction   Gold  \n",
       "0  At the point today where if someone says somet...      anger  anger  \n",
       "1  @CorningFootball  IT'S GAME DAY!!!!      T MIN...      anger  anger  \n",
       "2  This game has pissed me off more than any othe...      anger  anger  \n",
       "3  @spamvicious I've just found out it's Candice ...      anger  anger  \n",
       "4  @moocowward @mrsajhargreaves @Melly77 @GaryBar...       fear  anger  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_probabilities_2A = svm_linear_clf_2A.predict_proba(test_tfidf_2A)\n",
    "\n",
    "pred_labels_2A = []\n",
    "for predicted_label in y_pred_svm_2A:\n",
    "    pred_labels_2A.append(label_encoder.classes_[predicted_label])\n",
    "\n",
    "gold_labels_2A = []\n",
    "for gold_label in test_classes:\n",
    "    gold_labels_2A.append(label_encoder.classes_[gold_label])\n",
    "\n",
    "result_frame2A = pd.DataFrame(pred_probabilities_2A*100, columns=label_encoder.classes_)\n",
    "\n",
    "result_frame2A['Chat']= list(tweets_dftest['Tweet'])\n",
    "result_frame2A['Prediction']=pred_labels_2A\n",
    "result_frame2A['Gold']=gold_labels_2A\n",
    "\n",
    "result_frame2A.to_csv(\"result_frame2A.csv\")\n",
    "result_frame2A.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "558f5351-85b1-431d-9788-4ee0f4ea5780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most important features per emotion: 2A\n",
      "Important words in anger utterances\n",
      "anger 3.6531723579385096 anger\n",
      "anger 3.642727066735774 rage\n",
      "anger 3.1845846298112006 offend\n",
      "anger 3.125060604146694 bitter\n",
      "anger 3.112672764842934 angry\n",
      "anger 2.9477823412368043 revenge\n",
      "anger 2.7428586341517507 fury\n",
      "anger 2.741317666066721 offense\n",
      "anger 2.680995221601827 fume\n",
      "anger 2.597124900468489 snap\n",
      "anger 2.5961267143253854 burst\n",
      "anger 2.506765325183344 madden\n",
      "anger 2.4054276084847963 relentless\n",
      "anger 2.400482322595806 outrage\n",
      "anger 2.391648001364326 rabid\n",
      "anger 2.365590793005136 resent\n",
      "anger 2.347488282200855 wrath\n",
      "anger 2.283261928110445 burn\n",
      "anger 2.2735609888285153 irritate\n",
      "anger 2.225944378750616 insult\n",
      "-----------------------------------------\n",
      "Important words in fear utterances\n",
      "fear 3.125602040902827 terrorism\n",
      "fear 2.9986588971243657 shake\n",
      "fear 2.7834003795187847 horror\n",
      "fear 2.701618724341592 nightmare\n",
      "fear 2.7009975802301063 bully\n",
      "fear 2.6737671580995594 fear\n",
      "fear 2.6405042449598675 panic\n",
      "fear 2.5250751691175912 shy\n",
      "fear 2.4999041386930094 nervous\n",
      "fear 2.4853646032677723 awful\n",
      "fear 2.461523098577202 awe\n",
      "fear 2.451068358586995 terror\n",
      "fear 2.4445471945092385 horrible\n",
      "fear 2.3489495491527634 alarm\n",
      "fear 2.3426292631779866 terrible\n",
      "fear 2.3312897875405127 afraid\n",
      "fear 2.220713345384397 horrific\n",
      "fear 2.0798880689145207 start\n",
      "fear 2.0512112471106723 terrific\n",
      "fear 2.0015065189120587 dread\n",
      "-----------------------------------------\n",
      "Important words in joy utterances\n",
      "joy 3.4783252156308455 cheer\n",
      "joy 3.2019830291598836 playful\n",
      "joy 3.1733151354793465 rejoice\n",
      "joy 3.07824903204649 optimism\n",
      "joy 3.0571274429051942 elated\n",
      "joy 2.9357434679697425 glee\n",
      "joy 2.829273030445631 hilarious\n",
      "joy 2.769312121510884 joyous\n",
      "joy 2.7417226691484227 cheerfully\n",
      "joy 2.736739564330248 cheerful\n",
      "joy 2.599619170238175 smile\n",
      "joy 2.599203192531568 animate\n",
      "joy 2.5454236643864667 cheery\n",
      "joy 2.509572172463231 laughter\n",
      "joy 2.483657066685187 hearty\n",
      "joy 2.4835600506794338 delight\n",
      "joy 2.465710844307104 lively\n",
      "joy 2.4620370613761646 happy\n",
      "joy 2.4151617064222846 breezy\n",
      "joy 2.388264728597382 exhilarate\n",
      "-----------------------------------------\n",
      "Important words in sadness utterances\n",
      "sadness 3.668700746179101 serious\n",
      "sadness 3.390265132472133 unhappy\n",
      "sadness 3.291259442606255 sober\n",
      "sadness 3.24041324486897 dark\n",
      "sadness 3.1116807556110797 sink\n",
      "sadness 3.1054330247015463 depression\n",
      "sadness 3.0852114586168695 lose\n",
      "sadness 2.9682104549102872 grim\n",
      "sadness 2.8319750890162423 depress\n",
      "sadness 2.797550862090485 blue\n",
      "sadness 2.7755694445760826 depressing\n",
      "sadness 2.7540403145655232 dull\n",
      "sadness 2.731178363107037 discourage\n",
      "sadness 2.6561375714284914 pine\n",
      "sadness 2.6115234593157646 gloomy\n",
      "sadness 2.5767035650166505 sad\n",
      "sadness 2.5306090337144904 stay\n",
      "sadness 2.5186826980042714 sadly\n",
      "sadness 2.4399294150149573 mourn\n",
      "sadness 2.4269959561499674 weary\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def average_importances(model):\n",
    "    coef_avg = 0\n",
    "    for classifier in model.calibrated_classifiers_:\n",
    "        coef_avg = coef_avg + classifier.base_estimator.coef_\n",
    "        \n",
    "    coef_avg  = coef_avg/len(model.calibrated_classifiers_)\n",
    "    return coef_avg\n",
    "\n",
    "def f_importances(importances, names, n=20):\n",
    "    class_labels = label_encoder.classes_\n",
    "    \n",
    "    for num, imp in enumerate(importances):\n",
    "        emotion = class_labels[num]\n",
    "        topn = sorted(zip(imp,names), reverse=True)[:n]\n",
    "        \n",
    "        print(\"Important words in {} utterances\".format(emotion))\n",
    "        for coef, feat in topn:\n",
    "            print(emotion, coef, feat)\n",
    "        print(\"-----------------------------------------\")\n",
    "\n",
    "print('Most important features per emotion: 2A')\n",
    "feature_names = utterance_vec_2A.get_feature_names()\n",
    "importances = average_importances(svm_linear_clf_2A)\n",
    "f_importances(importances, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f8e622-4e4d-4f61-83eb-82c121f871dd",
   "metadata": {},
   "source": [
    "### Filter B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0c676bfc-cd46-453c-a707-68a503f8ad53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determiner and pronouns {'¬ª', '@xmaseveevil1', 'boys', 'no', 'this', '_', 'our', '@ryuredwings2', '\\\\nindia', '\\\\nmatt', '#', '@the', 'half', 'they', '@realdonaldtrump', 'myself', 'nj@latimes', '@messyourself', 'ios10', '@bbnicole', 'em', '@talktalkcare', \"\\\\n\\\\n'you\", 'tbh', 'lt', 'every', '@space_gayz', \"'s\", 'said!!!!\\\\nthey', 'each', 'its', \"y'\", 'üòø', '@aefadul22', '\\\\nso', '@ritujai18874', '@johnjharwood', 'he', 'a', 'both', 'these', '@jankhambrams', 'yours', 'her', '@eliroth', 'you', 'himself', 'either', 'tvgirl', \"naya'\\\\n\\\\n'i\", 'we', '#behaviour', 'that,\\\\ngives', 'that', '@sarahb45', 'neither', '@rosie', \"'em\", 'blm', 'mine', '@colinoccupantz', 'üí¶', 'lv', 'i', '@digger_forum', 'one', 'thy', '@jbanks88', 'those', 'yhat', '\\\\nwhat', 'ya', 'an', '@adele', '@capitalone', 'it.\\\\n#funny', 'y', 'another', '@barbour', '‚ú®', 'üòë', 'some', 'itself', 'the', '@your', 'themselves', 'his', 'my', 'isthereahelplineforthis', 'it', 'd', 'üçÇ', 'theirs', 'thee', 'your', 'she', 'u', '@interception225', '@barackobama', 'yourself', 'any', 'ours', 'their', 'all', '@jdegrom19', 'üòä', 'happy\\\\nshe', '@kristasaidthis', 'tho', 'ourselves', 'stupid?that', 'üòÑ'}\n",
      "Min_df 2\n"
     ]
    }
   ],
   "source": [
    "low_df_test_2B, DTandPRP_test_2B, clean_test_2B = \\\n",
    "remove_DT_PRP(2, test_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "31d05227-29b4-4f34-898a-6a01385559c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_count_2B = utterance_vec_2B.transform(clean_test_2B)\n",
    "test_tfidf_2B = tfidf_transformer.fit_transform(test_count_2B)\n",
    "\n",
    "y_pred_svm_2B = svm_linear_clf_2B.predict(test_tfidf_2B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fce4c065-36c1-47b2-87ce-3602e10dab31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger' 'fear' 'joy' 'sadness']\n",
      "BoW TFIDF SVM LINEAR: Tweets, Filter B\n",
      "Word mininum document frequency 2 ; DT PRP removed\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.837838  0.775000  0.805195       760\n",
      "           1   0.801628  0.791960  0.796764       995\n",
      "           2   0.841816  0.857143  0.849410       714\n",
      "           3   0.746228  0.808321  0.776034       673\n",
      "\n",
      "    accuracy                       0.806174      3142\n",
      "   macro avg   0.806877  0.808106  0.806851      3142\n",
      "weighted avg   0.807652  0.806174  0.806327      3142\n",
      "\n",
      "Confusion matrix SVM, BoW Tweets, Filter B\n",
      "['anger' 'fear' 'joy' 'sadness']\n",
      "[[589  77  32  62]\n",
      " [ 55 788  55  97]\n",
      " [ 16  60 612  26]\n",
      " [ 43  58  28 544]]\n"
     ]
    }
   ],
   "source": [
    "report2B = classification_report(test_classes,y_pred_svm_2B,digits = 4)\n",
    "print(label_encoder.classes_)\n",
    "print('BoW TFIDF SVM LINEAR: Tweets, Filter B')\n",
    "print('Word mininum document frequency', min_df, \"; DT PRP removed\")\n",
    "print(report2B)\n",
    "\n",
    "print('Confusion matrix SVM, BoW Tweets, Filter B')\n",
    "print(label_encoder.classes_)\n",
    "print(sklearn.metrics.confusion_matrix(test_classes,y_pred_svm_2B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4813af15-5dfb-4fc4-901a-e8227a49a07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probabilities_2B = svm_linear_clf_2B.predict_proba(test_tfidf_2B)\n",
    "\n",
    "pred_labels_2B = []\n",
    "for predicted_label in y_pred_svm_2B:\n",
    "    pred_labels_2B.append(label_encoder.classes_[predicted_label])\n",
    "\n",
    "gold_labels_2B = []\n",
    "for gold_label in test_classes:\n",
    "    gold_labels_2B.append(label_encoder.classes_[gold_label])\n",
    "\n",
    "result_frame2B = pd.DataFrame(pred_probabilities_2B*100, columns=label_encoder.classes_)\n",
    "\n",
    "result_frame2B['Chat']= list(tweets_dftest['Tweet'])\n",
    "result_frame2B['Prediction']=pred_labels_2B\n",
    "result_frame2B['Gold']=gold_labels_2B\n",
    "\n",
    "result_frame2B.to_csv(\"result_frame2B.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "63f6364d-ae9c-4357-b210-1ab06ba85cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anger</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>Chat</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Gold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80.252924</td>\n",
       "      <td>18.248005</td>\n",
       "      <td>0.489399</td>\n",
       "      <td>1.009673</td>\n",
       "      <td>At the point today where if someone says somet...</td>\n",
       "      <td>anger</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93.832984</td>\n",
       "      <td>2.342541</td>\n",
       "      <td>1.199966</td>\n",
       "      <td>2.624508</td>\n",
       "      <td>@CorningFootball  IT'S GAME DAY!!!!      T MIN...</td>\n",
       "      <td>anger</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.266340</td>\n",
       "      <td>1.443278</td>\n",
       "      <td>0.373367</td>\n",
       "      <td>9.917015</td>\n",
       "      <td>This game has pissed me off more than any othe...</td>\n",
       "      <td>anger</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45.246627</td>\n",
       "      <td>2.054475</td>\n",
       "      <td>2.466968</td>\n",
       "      <td>50.231930</td>\n",
       "      <td>@spamvicious I've just found out it's Candice ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.800960</td>\n",
       "      <td>55.035215</td>\n",
       "      <td>0.928871</td>\n",
       "      <td>3.234954</td>\n",
       "      <td>@moocowward @mrsajhargreaves @Melly77 @GaryBar...</td>\n",
       "      <td>fear</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       anger       fear       joy    sadness  \\\n",
       "0  80.252924  18.248005  0.489399   1.009673   \n",
       "1  93.832984   2.342541  1.199966   2.624508   \n",
       "2  88.266340   1.443278  0.373367   9.917015   \n",
       "3  45.246627   2.054475  2.466968  50.231930   \n",
       "4  40.800960  55.035215  0.928871   3.234954   \n",
       "\n",
       "                                                Chat Prediction   Gold  \n",
       "0  At the point today where if someone says somet...      anger  anger  \n",
       "1  @CorningFootball  IT'S GAME DAY!!!!      T MIN...      anger  anger  \n",
       "2  This game has pissed me off more than any othe...      anger  anger  \n",
       "3  @spamvicious I've just found out it's Candice ...    sadness  anger  \n",
       "4  @moocowward @mrsajhargreaves @Melly77 @GaryBar...       fear  anger  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_frame2B.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6073a634-b02d-4327-87b7-9b1bd5583208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most important features per emotion for the SVM classifier\n",
      "Important words in anger utterances\n",
      "anger 3.8134984964311327 anger\n",
      "anger 3.786781189451753 rage\n",
      "anger 3.230216875654725 bitter\n",
      "anger 3.2152211955083283 offend\n",
      "anger 3.213678089849894 angry\n",
      "anger 3.0240640140831125 revenge\n",
      "anger 2.772870201902399 fume\n",
      "anger 2.7264526576923207 fury\n",
      "anger 2.7004319275203663 offense\n",
      "anger 2.6379413900302477 burst\n",
      "anger 2.606032670892099 snap\n",
      "anger 2.5410851452003143 madden\n",
      "anger 2.5362554842308036 relentless\n",
      "anger 2.477317025828108 rabid\n",
      "anger 2.408564394524163 wrath\n",
      "anger 2.4068473518754994 outrage\n",
      "anger 2.3591358697076643 burn\n",
      "anger 2.254733474390254 insult\n",
      "anger 2.253847730574333 irritate\n",
      "anger 2.250037624745847 resent\n",
      "-----------------------------------------\n",
      "Important words in fear utterances\n",
      "fear 3.21861375239446 terrorism\n",
      "fear 3.138634628898958 shake\n",
      "fear 2.894621260276061 horror\n",
      "fear 2.769012346630014 bully\n",
      "fear 2.752972084147799 fear\n",
      "fear 2.728016022227951 nightmare\n",
      "fear 2.7110650521048485 panic\n",
      "fear 2.5743016053284538 awful\n",
      "fear 2.564428841013317 shy\n",
      "fear 2.5561481798745724 awe\n",
      "fear 2.5481226566935176 horrible\n",
      "fear 2.542313421441368 nervous\n",
      "fear 2.459452535130054 terror\n",
      "fear 2.417636448256361 terrible\n",
      "fear 2.402044360451785 alarm\n",
      "fear 2.3155656853602014 afraid\n",
      "fear 2.304408375675186 horrific\n",
      "fear 2.107583215841699 terrific\n",
      "fear 2.0934923754499337 start\n",
      "fear 2.0480488241462544 worry\n",
      "-----------------------------------------\n",
      "Important words in joy utterances\n",
      "joy 3.5959514293054724 cheer\n",
      "joy 3.176723791320778 optimism\n",
      "joy 3.06669479743111 rejoice\n",
      "joy 3.0560957960626594 elated\n",
      "joy 3.0213135646573126 playful\n",
      "joy 3.0074173008537413 glee\n",
      "joy 2.9116342584469543 hilarious\n",
      "joy 2.832279546062786 smile\n",
      "joy 2.7452230923555034 cheerfully\n",
      "joy 2.728275919755793 joyous\n",
      "joy 2.7094692971317333 cheerful\n",
      "joy 2.621579656172253 animate\n",
      "joy 2.5971435546953385 happy\n",
      "joy 2.538723603478802 laughter\n",
      "joy 2.5273365058038686 delight\n",
      "joy 2.516460285487555 hearty\n",
      "joy 2.486535591149014 cheery\n",
      "joy 2.448667055265203 lively\n",
      "joy 2.4410991265108324 breezy\n",
      "joy 2.4200556315840407 hilarity\n",
      "-----------------------------------------\n",
      "Important words in sadness utterances\n",
      "sadness 3.6478236443555536 serious\n",
      "sadness 3.4377809255513734 unhappy\n",
      "sadness 3.3490879351083245 sober\n",
      "sadness 3.285229767537042 sink\n",
      "sadness 3.282306368340823 dark\n",
      "sadness 3.1442408007370166 lose\n",
      "sadness 3.08969038697236 depression\n",
      "sadness 3.012375667131673 grim\n",
      "sadness 2.932886531875234 blue\n",
      "sadness 2.8614656270338967 depress\n",
      "sadness 2.8515559481817725 depressing\n",
      "sadness 2.840775236118488 dull\n",
      "sadness 2.807680100296515 pine\n",
      "sadness 2.7388205771627847 discourage\n",
      "sadness 2.7012489883700015 sad\n",
      "sadness 2.6675952388603683 stay\n",
      "sadness 2.649753560996523 gloomy\n",
      "sadness 2.5301808045528493 sadly\n",
      "sadness 2.4346927713175353 mourn\n",
      "sadness 2.429590994613169 weary\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('Most important features per emotion for the SVM classifier')\n",
    "feature_names = utterance_vec_2B.get_feature_names()\n",
    "importances = average_importances(svm_linear_clf_2B)\n",
    "f_importances(importances, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fd9f3d-c5a9-4380-a2f1-4b4ed3ead60b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
