{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c09547e-9a7d-4fb5-94ca-1e87df66694b",
   "metadata": {},
   "source": [
    "# 1. MELD, BoW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edd1c61-f66c-460b-931d-6e807aa337bc",
   "metadata": {},
   "source": [
    "## 1.1 Data preparation and filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e247abb7-7266-4851-8a4a-44d1c3c5bf6d",
   "metadata": {},
   "source": [
    "(a) Loading the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ad9f17a-60a3-4f42-a3ad-601c88d577a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c7a0c11-8289-452b-a315-ccf06e14cfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-6ff697eb93e3>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  meld_dftrain['Utterance'] = meld_dftrain['Utterance'].str.replace(\"\\x92|\\x97|\\x91|\\x93|\\x94|\\x85\", \"'\")\n",
      "<ipython-input-2-6ff697eb93e3>:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  meld_dftest['Utterance'] = meld_dftest['Utterance'].str.replace(\"\\x92|\\x97|\\x91|\\x93|\\x94|\\x85\", \"'\")\n"
     ]
    }
   ],
   "source": [
    "filepath = './data/MELD/train_sent_emo.csv'\n",
    "meld_dftrain = pd.read_csv(filepath)\n",
    "meld_dftrain['Utterance'] = meld_dftrain['Utterance'].str.replace(\"\\x92|\\x97|\\x91|\\x93|\\x94|\\x85\", \"'\")\n",
    "\n",
    "filepath = './data/MELD/test_sent_emo.csv'\n",
    "meld_dftest = pd.read_csv(filepath)\n",
    "meld_dftest['Utterance'] = meld_dftest['Utterance'].str.replace(\"\\x92|\\x97|\\x91|\\x93|\\x94|\\x85\", \"'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc1d654-5cbd-4766-b17e-4607a3253632",
   "metadata": {},
   "source": [
    "(b) MELD data preprocessing: removing 'Neutral' utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c21865a-e3d4-46e4-8fef-33a4064cd542",
   "metadata": {},
   "outputs": [],
   "source": [
    "meld_dftrain = meld_dftrain.set_index(\"Emotion\", drop=False)\n",
    "meld_dftrain = meld_dftrain.drop(\"neutral\", axis=0)\n",
    "\n",
    "meld_dftest = meld_dftest.set_index(\"Emotion\", drop=False)\n",
    "meld_dftest = meld_dftest.drop(\"neutral\", axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8d33267-a2de-4d98-b69e-4e3f1d82b1d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My duties?  All right.',\n",
       " \"No don't I beg of you!\",\n",
       " 'Really?!',\n",
       " 'But then who? The waitress I went out with last month?',\n",
       " 'You know? Forget it!',\n",
       " 'No-no-no-no, no! Who, who were you talking about?',\n",
       " \"No, I-I-I-I don't, I actually don't know\",\n",
       " 'Do I ever.',\n",
       " \"Chris says they're closing down the bar.\",\n",
       " 'No way!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(meld_dftrain['Utterance'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265b3585-bd88-4828-a4bc-a25d00695575",
   "metadata": {},
   "source": [
    "(c) Building the filters.\n",
    "These filters are written here, and saved in utils.py."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565c7e07-6d64-4235-8531-b85c0e9006a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Filter A: MaxDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ad02ef8-e9d8-4a27-8bcf-40029cdb5f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_high_mid_df(min_df, max_df, texts):\n",
    "    \"\"\"\n",
    "    This function separates texts into three iterables according to their document frequencies.\n",
    "    :min_df: int, words occuring in documents less than this number will be put into low_df.\n",
    "    :max_df: int, words occuring in documents more than this number will be put into high_df.\n",
    "    :texts: a list of lists with sentences of spaCy-tokenized words.\n",
    "    :return: low_df - set of words of which df is too low.\n",
    "        high_df - set of words of which df is too high.\n",
    "        mid_df_texts - list of texts in which the dfs of their words are in the middle.\n",
    "    \"\"\"\n",
    "    new_texts = []\n",
    "    alltokens = set()\n",
    "    for text in texts:\n",
    "        sent = []\n",
    "        for token in text:\n",
    "            token = token.lemma_.lower()  # Lemmatize the input and make them lowercase.\n",
    "            sent.append(token)\n",
    "            alltokens.add(token)\n",
    "        new_texts.append(sent)\n",
    "    \n",
    "    kw_count = dict.fromkeys(alltokens, 0)\n",
    "    for text in new_texts:\n",
    "        for key in kw_count:\n",
    "            if key in text:  # If a word is in a document,\n",
    "                kw_count[key] += 1  # Df += 1\n",
    "\n",
    "    low_df = set()\n",
    "    high_df = set()\n",
    "    for word, count in kw_count.items():\n",
    "        if count>max_df:\n",
    "            high_df.add(word)\n",
    "        elif count<min_df:\n",
    "            low_df.add(word)\n",
    "            \n",
    "    mid_df_texts = []\n",
    "    for text in new_texts:\n",
    "        mid_df_texts.append([tok for tok in text if (tok not in high_df) and (tok not in low_df)])\n",
    "    \n",
    "    print('Min_df', min_df)\n",
    "    print('Max_df', max_df)\n",
    "    return low_df, high_df, mid_df_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de6dfa29-e865-43db-8a8a-b1ee025e6a82",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min_df 2\n",
      "Max_df 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'(',\n",
       "  ')',\n",
       "  'a',\n",
       "  'bad',\n",
       "  'blood',\n",
       "  'but',\n",
       "  'defeat',\n",
       "  'macho',\n",
       "  'man',\n",
       "  \"n't\",\n",
       "  'one',\n",
       "  'run',\n",
       "  'see',\n",
       "  'to',\n",
       "  'tough',\n",
       "  'want'},\n",
       " {',', 'be'},\n",
       " [['so', 'beat', 'it', 'just', 'beat', 'it'],\n",
       "  ['you', 'well', 'you', 'well', 'do', 'what', 'you', 'can'],\n",
       "  ['do', 'wanna', 'no', 'do'],\n",
       "  ['you', 'wanna', 'well', 'do', 'what', 'you', 'can'],\n",
       "  ['so', 'beat', 'it', 'you', 'wanna'],\n",
       "  ['just', 'beat', 'it', 'beat', 'it', 'beat', 'it', 'beat', 'it'],\n",
       "  ['no']])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A test for the filter\n",
    "test_str_a = [\"So beat it, just beat it\",\n",
    "\"You better run, you better do what you can\",\n",
    "\"Don't wanna see no blood, don't be a macho man\",\n",
    "\"You wanna be tough, better do what you can\",\n",
    "\"So beat it, but you wanna be bad\",\n",
    "\"Just beat it (beat it), beat it (beat it)\",\n",
    "\"No one wants to be defeated\"]\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "test_list_a = [nlp(sent) for sent in test_str_a]\n",
    "\n",
    "low_high_mid_df(2, 3, test_list_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a8e187-c151-47e6-a6a4-18a004d2c9d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Filter B: DTandPRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83de530d-4eea-483e-973b-9e5a5b372386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_DT_PRP(min_df, texts):\n",
    "    \"\"\"\n",
    "    This function 1) removes determiners and pronouns in texts; 2) separates rare words with low df.\n",
    "    :min_df: int, words occuring in documents less than this number will be put into low_df.\n",
    "    :texts: a list of lists with sentences of spaCy-tokenized, POS-tagged words.\n",
    "    :return: low_df - list of words of which df is too low.\n",
    "        clean_texts - list of sentences with no determiners, pronouns and low-df words.\n",
    "    \"\"\"\n",
    "    DTandPRP_tag = [\"DT\", \"PRP\", \"PRP$\"]\n",
    "    DTandPRP_tok = set()\n",
    "    vocab = set()\n",
    "    new_texts = []\n",
    "    for text in texts:\n",
    "        sent = []\n",
    "        for token in text:\n",
    "            if token.tag_ in DTandPRP_tag:\n",
    "                DTandPRP_tok.add(token.lemma_.lower())\n",
    "            else:\n",
    "                token = token.lemma_.lower()\n",
    "                sent.append(token)\n",
    "                vocab.add(token)  # Create a set a vocab without the DTs and PRPs\n",
    "        new_texts.append(sent)\n",
    "    \n",
    "    kw_count = dict.fromkeys(vocab, 0)\n",
    "    for text in new_texts:\n",
    "        for key in kw_count:\n",
    "            if key in text:  # If a word is in a document,\n",
    "                kw_count[key] += 1  # Df += 1\n",
    "    \n",
    "    low_df = set()\n",
    "    for word, count in kw_count.items():\n",
    "        if count<min_df:\n",
    "            low_df.add(word)\n",
    "    \n",
    "    clean_texts = []\n",
    "    for text in new_texts:\n",
    "        # Keep words if they are not DTs nor PRPs, and not lower than min_df\n",
    "        clean_texts.append([tok for tok in text if (tok in vocab) and (tok not in low_df)])\n",
    "    \n",
    "    print('Determiner and pronouns', DTandPRP_tok)\n",
    "    print('Min_df', min_df)\n",
    "    return low_df, DTandPRP_tok, clean_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "382b93db-cefd-4ebe-b4cb-1250765e46c6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determiner and pronouns {'it', 'a', 'no', 'you'}\n",
      "Min_df 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'(',\n",
       "  ')',\n",
       "  'bad',\n",
       "  'blood',\n",
       "  'but',\n",
       "  'defeat',\n",
       "  'macho',\n",
       "  'man',\n",
       "  \"n't\",\n",
       "  'one',\n",
       "  'run',\n",
       "  'see',\n",
       "  'to',\n",
       "  'tough',\n",
       "  'want'},\n",
       " {'a', 'it', 'no', 'you'},\n",
       " [['so', 'beat', ',', 'just', 'beat'],\n",
       "  ['well', ',', 'well', 'do', 'what', 'can'],\n",
       "  ['do', 'wanna', ',', 'do', 'be'],\n",
       "  ['wanna', 'be', ',', 'well', 'do', 'what', 'can'],\n",
       "  ['so', 'beat', ',', 'wanna', 'be'],\n",
       "  ['just', 'beat', 'beat', ',', 'beat', 'beat'],\n",
       "  ['be']])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A test for the filter\n",
    "test_str_b = [\"So beat it, just beat it\",\n",
    "\"You better run, you better do what you can\",\n",
    "\"Don't wanna see no blood, don't be a macho man\",\n",
    "\"You wanna be tough, better do what you can\",\n",
    "\"So beat it, but you wanna be bad\",\n",
    "\"Just beat it (beat it), beat it (beat it)\",\n",
    "\"No one wants to be defeated\"]\n",
    "\n",
    "test_list_b = [nlp(sent) for sent in test_str_b]\n",
    "\n",
    "remove_DT_PRP(2, test_list_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba49719-fffb-48d7-a8df-7f28acbee7b0",
   "metadata": {},
   "source": [
    "(d) Tokenizing and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe6eea66-fc87-450c-96d6-afc44f8004fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using spaCy to tokenize the sentences\n",
    "training_data_1 = [nlp(sent) for sent in list(meld_dftrain['Utterance'])]\n",
    "training_labels_1 = list(meld_dftrain['Emotion'])\n",
    "\n",
    "test_data_1 = [nlp(sent) for sent in list(meld_dftest['Utterance'])]\n",
    "test_labels_1 = list(meld_dftest['Emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da34b922-7685-48e7-b745-17a88cc17995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sent in training_data_1:\n",
    "#     for token in sent:\n",
    "#         if \"I'm\" == token.text:\n",
    "#             print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b91bab-11ae-4540-8542-43c01b7d6d32",
   "metadata": {},
   "source": [
    "    (i) Filter A: MaxDF\n",
    "\n",
    " - min_df would be 2, considering words appearing in less than 2 documents are rare.\n",
    " - max_df would be 1/10 of the amount of documents, which means if a word appear once in every 10 documents, its df is considered to be too high. Besides subjective and intuitive judgment, this number is also manipulated to make the vocabulary sizes of the two filters more similar and comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f367242c-6231-411a-993d-2ec6bbd9f456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min_df 2\n",
      "Max_df 527\n",
      "Rare words with low df =  1680 words. Examples:  ['address', 'scooter', 'swim', 'island', 'mellow', 'pink', 'yentel', 'paste', '33', 'mattress', 'tibidaybo', 'meat', 'confident', 'thin', 'swirl', 'adorable', 'requires', 'lord', 'frustrating', 'lay']\n",
      "Stop words with high df: {'and', '!', 'a', 'what', \"n't\", 'that', 'do', '?', 'the', 'you', 'it', 'to', '.', 'be', 'i', 'oh', ','}\n",
      "Size of the rest vocab: 1530\n",
      "Samples: [['just', 'coffee', 'where', 'we', 'gon', 'na', 'hang', 'out', 'now'], ['got'], [], ['um', '-', 'mm', 'yeah', 'right'], ['my', 'god', 'my', 'god', 'poor', 'monica'], [], [], ['he', 'think', 'monica', 'empty', 'she', 'empty', 'vase'], ['totally', 'god', 'she', 'seem', 'so', 'happy', 'too'], ['hey']]\n"
     ]
    }
   ],
   "source": [
    "#from utils import low_high_mid_df\n",
    "\n",
    "min_df = 2\n",
    "max_df = len(training_data_1)//10\n",
    "\n",
    "low_df, high_df, clean1A = low_high_mid_df(min_df, max_df, training_data_1)\n",
    "\n",
    "print(\"Rare words with low df = \", len(low_df), \"words. Examples: \", list(low_df)[:20])\n",
    "print(\"Stop words with high df:\", high_df)\n",
    "vocab_1A = set()\n",
    "for sent in clean1A:\n",
    "    for t in sent:\n",
    "        vocab_1A.add(t)\n",
    "print(\"Size of the rest vocab:\", len(vocab_1A))\n",
    "print(\"Samples:\", clean1A[10:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a43f3b-82f0-4a4b-8808-ce355f558703",
   "metadata": {},
   "source": [
    "    (ii) Filter B: DTandPRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e8e347d-a24f-47e5-bcf3-bdf3577a7cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determiner and pronouns {'some', \"underwear'you\", 'that', 'all', 'that?s', \"i'll\", 'they', \"mean'i\", \"was'the\", 'he', 'an', 'i', 'yourself', \"i'i'm\", 'those', 'every', 'both', 'its', 'ya', 'either', \"you're\", 'hers', 'herself', \"you're'you\", 'yours', 'ourselves', 'your', \"i'm\", \"fact'yes\", 'her', 'i-', 'his', 'we', 'ba', 'any', 'these', 'a', \"it's\", \"up'i\", 'you', 'each', 'my', 'no', 'our', 'themselves', \"that'you\", \"'em\", 'myself', \"film'that\", 'itself', 'tux', 'mine', 'this', 'the', 'their', 'ours', 'it', 'another', 'the-', \"i'y'know\", \"'s\", 'she', 'neither'}\n",
      "Min_df 2\n",
      "Rare words with low df =  1670 words. Examples: ['address', 'scooter', 'swim', 'island', 'mellow', 'pink', 'yentel', 'paste', '33', 'mattress', 'tibidaybo', 'meat', 'confident', 'thin', 'swirl', 'adorable', 'requires', 'lord', 'frustrating', 'lay']\n",
      "Size of the rest vocab: 1515\n",
      "Samples: [['just', 'coffee', '!', 'where', 'be', 'gon', 'na', 'hang', 'out', 'now', '?'], ['got', '.'], ['!'], ['um', '-', 'mm', ',', 'yeah', 'right', '!'], ['oh', 'my', 'god', ',', 'oh', 'my', 'god', '!', 'poor', 'monica', '!'], ['what', ',', 'what', ',', 'what', '?', '!'], ['what', '?', '!'], ['think', 'monica', 'be', 'empty', ',', 'be', 'empty', 'vase', '!'], ['oh', ',', 'totally', '.', 'oh', ',', 'god', ',', 'oh', ',', 'seem', 'so', 'happy', 'too', '.'], ['hey', '!']]\n"
     ]
    }
   ],
   "source": [
    "#from utils import remove_DT_PRP\n",
    "\n",
    "min_df = 2\n",
    "\n",
    "low_df, DTandPRP_tok, clean1B = remove_DT_PRP(min_df, training_data_1)\n",
    "\n",
    "print(\"Rare words with low df = \", len(low_df), \"words. Examples:\", list(low_df)[:20])\n",
    "vocab_1B = set()\n",
    "for sent in clean1B:\n",
    "    for t in sent:\n",
    "        vocab_1B.add(t)\n",
    "print(\"Size of the rest vocab:\", len(vocab_1B))\n",
    "print(\"Samples:\", clean1B[10:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5503dcd-051f-456c-8c06-242dc3936abb",
   "metadata": {},
   "source": [
    "## 1.2 BoW vectorization and training the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98847da8-1120-4517-8908-39c1944c68b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f427cbe-cc5b-4315-89f6-6c8e4d092ddf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loaded_classifier = pickle.load(open(filename_classifier, 'rb'))\n",
    "# loaded_vectorizer = pickle.load(open(filename_vectorizer, 'rb'))\n",
    "# loaded_transformer = pickle.load(open(filename_transformer, 'rb'))\n",
    "# loaded_label_encoder = pickle.load(open(filename_encoder, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f462c6-2441-453b-ba1d-ab4c3625dd4e",
   "metadata": {},
   "source": [
    "(a) Encoding training labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48136334-9fc4-46d2-b1b4-be32e57d605f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit(training_labels_1+test_labels_1)\n",
    "print(list(label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62779ff8-6835-416f-8838-108b83cf6dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 2 5 5 4 5 2 3 4 5 1 4 3 3 5 5 5 5 4 5]\n",
      "['surprise', 'fear', 'surprise', 'surprise', 'sadness', 'surprise', 'fear', 'joy', 'sadness', 'surprise', 'disgust', 'sadness', 'joy', 'joy', 'surprise', 'surprise', 'surprise', 'surprise', 'sadness', 'surprise']\n",
      "['My duties?  All right.', \"No don't I beg of you!\", 'Really?!', 'But then who? The waitress I went out with last month?', 'You know? Forget it!', 'No-no-no-no, no! Who, who were you talking about?', \"No, I-I-I-I don't, I actually don't know\", 'Do I ever.', \"Chris says they're closing down the bar.\", 'No way!', 'Just coffee! Where are we gonna hang out now?', 'Got me.', 'You betcha!', 'Um-mm, yeah right!', 'Oh my God, oh my God! Poor Monica!', 'What, what, what?!', 'What?!', 'He thinks Monica is empty, she is the empty vase!', 'Oh, totally. Oh, God, oh, she seemed so happy too.', 'Hey!']\n"
     ]
    }
   ],
   "source": [
    "training_classes = label_encoder.transform(training_labels_1)\n",
    "print(training_classes[:20])\n",
    "print(list(meld_dftrain['Emotion'])[:20])\n",
    "print(list(meld_dftrain['Utterance'])[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a55ac9e-a8d7-4c54-89cb-4210170a94cb",
   "metadata": {},
   "source": [
    "### Filter A: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448d503f-ba83-4f94-9f89-d7d47f061500",
   "metadata": {},
   "source": [
    "(a) Vectorise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47f37672-b95f-4b50-ad7b-b24fc8ec1f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A CountVectorizer which takes tokenized lists as input\n",
    "### Taken from https://stackoverflow.com/questions/35867484/pass-tokens-to-countvectorizer,\n",
    "### https://stackoverflow.com/questions/27673527/how-should-i-vectorize-the-following-list-of-lists-with-scikit-learn, 26 Oct 2021\n",
    "\n",
    "def dummy(x):\n",
    "    return x\n",
    "\n",
    "utterance_vec_1A = CountVectorizer(tokenizer=dummy, lowercase=False)\n",
    "\n",
    "training_count_vectors_1A = utterance_vec_1A.fit_transform(clean1A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc898b49-b088-4d36-bf33-4e76cb2711d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(training_count_vectors_1A .toarray()[0][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48c15e0d-73ce-49b7-94b9-1ad6e5180fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1530\n"
     ]
    }
   ],
   "source": [
    "#Total number of word features or the length of the total vector\n",
    "print(len(utterance_vec_1A.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "216a60ec-b42f-4ab2-9489-14cba72e689f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '  ', '   ', '\"', '$', \"'\", \"'cause\", \"'d\", \"'em\", \"'ll\", \"'s\", \"'ve\", '(', '-', '--', '..', '...', '....', '.....', '1', '10', '15', '17', '18', '19', '2,000', '20', '200', '25', '40', '50', '500', '7', '700', '74', '80', ':', ';', '[', ']', 'aaron', 'able', 'about', 'absolutely', 'accent', 'accept', 'ace', 'across', 'act', 'actor']\n"
     ]
    }
   ],
   "source": [
    "# First 50 feature names\n",
    "print(list(utterance_vec_1A.get_feature_names())[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9f5e12c-8c97-4596-8b32-b9bd7cbd1b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert raw frequency counts into TF-IDF values\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "training_tfidf_1A = tfidf_transformer.fit_transform(training_count_vectors_1A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8b88474-444a-492c-9f4c-da5f4379221d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.47132701 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.53124026 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(training_tfidf_1A.toarray()[0][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96903b2b-b969-4e1a-9ed1-03507616e648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CalibratedClassifierCV(base_estimator=LinearSVC(max_iter=2000), cv=10)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "linear_model = svm.LinearSVC(max_iter=2000)\n",
    "svm_linear_clf_1A = CalibratedClassifierCV(linear_model , method='sigmoid', cv=10)\n",
    "\n",
    "svm_linear_clf_1A.fit(training_tfidf_1A, training_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a142e14-9ebe-4d66-944e-addc4eafbfeb",
   "metadata": {},
   "source": [
    "### Filter B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cd335d-3faa-47bf-85d9-7f8e00df61e9",
   "metadata": {},
   "source": [
    "(a) Vectorise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b77f2d46-1103-409f-9ebe-00368c7b589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance_vec_1B = CountVectorizer(tokenizer=dummy, lowercase=False)\n",
    "\n",
    "training_count_vectors_1B = utterance_vec_1B.fit_transform(clean1B)\n",
    "training_tfidf_1B = tfidf_transformer.fit_transform(training_count_vectors_1B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f2f7511-9811-4f96-9014-6058acca3a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1515\n"
     ]
    }
   ],
   "source": [
    "#Total number of word features or the length of the total vector\n",
    "print(len(utterance_vec_1B.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf812ed1-8047-47bc-92c7-735adcbec91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['personal', 'pete', 'pheebs', 'phil', 'phoebe', 'phone', 'pick', 'picnic', 'picture', 'pie']\n"
     ]
    }
   ],
   "source": [
    "# First 50 feature names\n",
    "print(list(utterance_vec_1B.get_feature_names())[1000:1010])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001ea6e4-7962-4024-9a90-a0faeed5e730",
   "metadata": {},
   "source": [
    "(b) Train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd6d36be-6001-4d01-a8a3-2864c686dece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CalibratedClassifierCV(base_estimator=LinearSVC(max_iter=2000), cv=10)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model = svm.LinearSVC(max_iter=2000)\n",
    "svm_linear_clf_1B = CalibratedClassifierCV(linear_model , method='sigmoid', cv=10)\n",
    "\n",
    "svm_linear_clf_1B.fit(training_tfidf_1B, training_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1bd11e-6826-4dbe-aedc-41c43bd7d6e1",
   "metadata": {},
   "source": [
    "## 1.3 Predicting the test data and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "baaa8b7e-e62b-429a-9991-c2afec624f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2646a9e2-5568-4801-934d-2582f38ea306",
   "metadata": {},
   "source": [
    "Encode the test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ccc6f43a-505d-4ccb-a298-42fc21361648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 3 3 3 3 3 3 3 4 5 0 0 0 3 3 2 0 1 5]\n",
      "['surprise', 'anger', 'joy', 'joy', 'joy', 'joy', 'joy', 'joy', 'joy', 'sadness', 'surprise', 'anger', 'anger', 'anger', 'joy', 'joy', 'fear', 'anger', 'disgust', 'surprise']\n",
      "[\"Why do all you're coffee mugs have numbers on the bottom?\", \"Oh. That's so Monica can keep track. That way if one on them is missing, she can be like, 'Where's number 27?!'\", 'Push!', \"Push 'em out, push 'em out, harder, harder.\", \"Push 'em out, push 'em out, way out!\", \"Let's get that ball and really move, hey, hey, ho, ho.\", \"Let's'  I was just'yeah, right.\", 'Push!', 'Push!', \"Uhh, yes I did but there isn't. Okay, here we go.\", 'Okay, go left. Left! Left!', \"Okay, y'know what? There is no more left, left!\", 'Oh okay, lift it straight up over your head!', 'Straight up over your head!', 'You can do it!', 'You can do it!', \"No wait, look. Look! I'm sorry, it's just I've never even\", 'Okay, fine, whatever. Welcome to the building.', 'Ugh, can you believe that guy!', 'Ohh!']\n"
     ]
    }
   ],
   "source": [
    "test_classes = label_encoder.transform(test_labels_1)\n",
    "print(test_classes[:20])\n",
    "print(list(meld_dftest['Emotion'])[:20])\n",
    "print(list(meld_dftest['Utterance'])[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a547d4-e641-4de4-a4de-b9e792c339e2",
   "metadata": {},
   "source": [
    "### Filter A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6c2325e2-cbb8-4203-a8d1-82170e549670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min_df 2\n",
      "Max_df 135\n"
     ]
    }
   ],
   "source": [
    "max_df_test = len(test_data_1)//10\n",
    "\n",
    "low_df_test_1A, high_df_test_1A, test_mid_df_1A = \\\n",
    "low_high_mid_df(2, max_df_test, test_data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5f325163-4e4f-4d40-93c8-842b5670ca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_count_1A = utterance_vec_1A.transform(test_mid_df_1A)\n",
    "test_tfidf_1A = tfidf_transformer.fit_transform(test_count_1A)\n",
    "\n",
    "y_pred_svm_1A = svm_linear_clf_1A.predict(test_tfidf_1A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "48c3259d-d9e0-491b-b758-718c4a8b9a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger' 'disgust' 'fear' 'joy' 'sadness' 'surprise']\n",
      "BoW TFIDF SVM LINEAR: MELD, Filter A\n",
      "Word mininum document frequency: 2; maximum: 135\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.418803  0.284058  0.338515       345\n",
      "           1   0.500000  0.014706  0.028571        68\n",
      "           2   0.500000  0.020000  0.038462        50\n",
      "           3   0.430636  0.741294  0.544790       402\n",
      "           4   0.432432  0.153846  0.226950       208\n",
      "           5   0.437143  0.544484  0.484945       281\n",
      "\n",
      "    accuracy                       0.430576      1354\n",
      "   macro avg   0.453169  0.293065  0.277039      1354\n",
      "weighted avg   0.435292  0.430576  0.386362      1354\n",
      "\n",
      "Confusion matrix SVM, BoW MELD, Filter A\n",
      "['anger' 'disgust' 'fear' 'joy' 'sadness' 'surprise']\n",
      "[[ 98   1   0 150  11  85]\n",
      " [ 21   1   0  22   4  20]\n",
      " [ 13   0   1  24   4   8]\n",
      " [ 38   0   0 298  12  54]\n",
      " [ 39   0   1 106  32  30]\n",
      " [ 25   0   0  92  11 153]]\n"
     ]
    }
   ],
   "source": [
    "report1A = classification_report(test_classes,y_pred_svm_1A,digits = 6)\n",
    "print(label_encoder.classes_)\n",
    "print('BoW TFIDF SVM LINEAR: MELD, Filter A')\n",
    "print(f'Word mininum document frequency: {min_df}; maximum: {max_df_test}')\n",
    "print(report1A)\n",
    "\n",
    "print('Confusion matrix SVM, BoW MELD, Filter A')\n",
    "print(label_encoder.classes_)\n",
    "print(sklearn.metrics.confusion_matrix(test_classes,y_pred_svm_1A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4d6dfac1-fa22-4750-821b-2828107a92dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>Chat</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Gold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.956565</td>\n",
       "      <td>5.370529</td>\n",
       "      <td>8.362130</td>\n",
       "      <td>22.724746</td>\n",
       "      <td>7.195511</td>\n",
       "      <td>41.390519</td>\n",
       "      <td>Why do all you're coffee mugs have numbers on ...</td>\n",
       "      <td>surprise</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.956232</td>\n",
       "      <td>3.814577</td>\n",
       "      <td>6.349448</td>\n",
       "      <td>48.573109</td>\n",
       "      <td>14.965021</td>\n",
       "      <td>4.341613</td>\n",
       "      <td>Oh. That's so Monica can keep track. That way ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.088784</td>\n",
       "      <td>4.879630</td>\n",
       "      <td>4.239784</td>\n",
       "      <td>28.195862</td>\n",
       "      <td>9.667696</td>\n",
       "      <td>33.928244</td>\n",
       "      <td>Push!</td>\n",
       "      <td>surprise</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.732127</td>\n",
       "      <td>7.285264</td>\n",
       "      <td>4.734900</td>\n",
       "      <td>28.663558</td>\n",
       "      <td>33.072693</td>\n",
       "      <td>6.511459</td>\n",
       "      <td>Push 'em out, push 'em out, harder, harder.</td>\n",
       "      <td>sadness</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.146414</td>\n",
       "      <td>6.687672</td>\n",
       "      <td>4.101298</td>\n",
       "      <td>35.686163</td>\n",
       "      <td>11.287242</td>\n",
       "      <td>19.091210</td>\n",
       "      <td>Push 'em out, push 'em out, way out!</td>\n",
       "      <td>joy</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       anger   disgust      fear        joy    sadness   surprise  \\\n",
       "0  14.956565  5.370529  8.362130  22.724746   7.195511  41.390519   \n",
       "1  21.956232  3.814577  6.349448  48.573109  14.965021   4.341613   \n",
       "2  19.088784  4.879630  4.239784  28.195862   9.667696  33.928244   \n",
       "3  19.732127  7.285264  4.734900  28.663558  33.072693   6.511459   \n",
       "4  23.146414  6.687672  4.101298  35.686163  11.287242  19.091210   \n",
       "\n",
       "                                                Chat Prediction      Gold  \n",
       "0  Why do all you're coffee mugs have numbers on ...   surprise  surprise  \n",
       "1  Oh. That's so Monica can keep track. That way ...        joy     anger  \n",
       "2                                              Push!   surprise       joy  \n",
       "3        Push 'em out, push 'em out, harder, harder.    sadness       joy  \n",
       "4               Push 'em out, push 'em out, way out!        joy       joy  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_probabilities_1A = svm_linear_clf_1A.predict_proba(test_tfidf_1A)\n",
    "\n",
    "pred_labels_1A = []\n",
    "for predicted_label in y_pred_svm_1A:\n",
    "    pred_labels_1A.append(label_encoder.classes_[predicted_label])\n",
    "\n",
    "gold_labels_1A = []\n",
    "for gold_label in test_classes:\n",
    "    gold_labels_1A.append(label_encoder.classes_[gold_label])\n",
    "\n",
    "result_frame1A = pd.DataFrame(pred_probabilities_1A*100, columns=label_encoder.classes_)\n",
    "\n",
    "result_frame1A['Chat']= list(meld_dftest['Utterance'])\n",
    "result_frame1A['Prediction']=pred_labels_1A\n",
    "result_frame1A['Gold']=gold_labels_1A\n",
    "\n",
    "result_frame1A.to_csv(\"result_frame1A.csv\")\n",
    "result_frame1A.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "558f5351-85b1-431d-9788-4ee0f4ea5780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most important features per emotion: 1A\n",
      "Important words in anger utterances\n",
      "anger 1.7819809207908481 tape\n",
      "anger 1.5872583100466553 mad\n",
      "anger 1.5782512719487427 calm\n",
      "anger 1.5536727080819999 oboe\n",
      "anger 1.4735916728142802 knuckle\n",
      "anger 1.4526197530277458 garbage\n",
      "anger 1.4518083936438988 quarter\n",
      "anger 1.429492054025632 gimme\n",
      "anger 1.4191189726675855 york\n",
      "anger 1.3879065982043068 fancy\n",
      "anger 1.3569580668773875 easy\n",
      "anger 1.3538630374952905 pack\n",
      "anger 1.3516010026819918 bloody\n",
      "anger 1.344249194703592 table\n",
      "anger 1.3397681910913177 jackass\n",
      "anger 1.3272354867985725 scrud\n",
      "anger 1.3195553028345093 choice\n",
      "anger 1.3099244715665272 care\n",
      "anger 1.3024775420366317 laminate\n",
      "anger 1.2971931691062708 suggestion\n",
      "-----------------------------------------\n",
      "Important words in disgust utterances\n",
      "disgust 1.774723789387489 disgusting\n",
      "disgust 1.7699263786385646 ew\n",
      "disgust 1.7391020527921757 violate\n",
      "disgust 1.6597005426841491 behave\n",
      "disgust 1.634613306739066 eww\n",
      "disgust 1.5211811408468356 boat\n",
      "disgust 1.495124304407058 clinic\n",
      "disgust 1.4460655885827725 drug\n",
      "disgust 1.3801653420796458 ai\n",
      "disgust 1.3801616897358593 mindy\n",
      "disgust 1.3537503841979028 ugh\n",
      "disgust 1.3314615453003846 toilet\n",
      "disgust 1.318923269601369 pig\n",
      "disgust 1.303973939280204 hate\n",
      "disgust 1.244321558405471 shame\n",
      "disgust 1.2165013918259833 smell\n",
      "disgust 1.2164914546196697 typical\n",
      "disgust 1.2135520669590882 body\n",
      "disgust 1.17683887280544 hairy\n",
      "disgust 1.1466544042342224 evil\n",
      "-----------------------------------------\n",
      "Important words in fear utterances\n",
      "fear 2.021799529784933 scared\n",
      "fear 1.7919335661234235 act\n",
      "fear 1.6913781469954905 judge\n",
      "fear 1.251857930748987 conscious\n",
      "fear 1.23520758572324 rip\n",
      "fear 1.213815051877852 possibly\n",
      "fear 1.1918100492616466 purse\n",
      "fear 1.191081938633348 swear\n",
      "fear 1.182154550054527 twelve\n",
      "fear 1.1576998943800285 treeger\n",
      "fear 1.0878209190939172 stomach\n",
      "fear 1.0423496792204259 uh\n",
      "fear 1.0310096308779473 hurt\n",
      "fear 1.0142145860842622 gun\n",
      "fear 1.0104668466090092 brave\n",
      "fear 0.9855285850321062 sir\n",
      "fear 0.9785699935028045 scary\n",
      "fear 0.9561067731866697 brochure\n",
      "fear 0.9487991797130245 jack\n",
      "fear 0.9483978474756034 fear\n",
      "-----------------------------------------\n",
      "Important words in joy utterances\n",
      "joy 1.8547091023117623 glad\n",
      "joy 1.8347751592054287 love\n",
      "joy 1.7923152402208786 yay\n",
      "joy 1.6652665197896925 great\n",
      "joy 1.6199511286405486 alan\n",
      "joy 1.6034450278337986 amazing\n",
      "joy 1.577550475843173 decide\n",
      "joy 1.5668552772349362 fun\n",
      "joy 1.521979256756871 tomorrow\n",
      "joy 1.515767754277646 thank\n",
      "joy 1.483015722680992 sail\n",
      "joy 1.4486210393569467 isabella\n",
      "joy 1.4472106249187302 magic\n",
      "joy 1.4303642865482142 incredible\n",
      "joy 1.4220588878493998 cool\n",
      "joy 1.4168194488014407 sweet\n",
      "joy 1.4105165964534818 score\n",
      "joy 1.405138730438547 ho\n",
      "joy 1.3533613491321006 summer\n",
      "joy 1.3509710061114972 worker\n",
      "-----------------------------------------\n",
      "Important words in sadness utterances\n",
      "sadness 1.9508044607617792 sorry\n",
      "sadness 1.828923895660754 band\n",
      "sadness 1.6883138719312434 side\n",
      "sadness 1.6039841833959407 club\n",
      "sadness 1.5051950667296619 hard\n",
      "sadness 1.4517075796482113 group\n",
      "sadness 1.4434152020657074 alone\n",
      "sadness 1.442612258294298 exist\n",
      "sadness 1.4245221849197987 hurry\n",
      "sadness 1.4221522006186766 sad\n",
      "sadness 1.421398686350557 awful\n",
      "sadness 1.4078162038576145 n’t\n",
      "sadness 1.404186667550603 estelle\n",
      "sadness 1.4020734000552137 guess\n",
      "sadness 1.3984928639713843 chippy\n",
      "sadness 1.3915899935300278 story\n",
      "sadness 1.3896101844366846 worry\n",
      "sadness 1.3838390478208524 puppet\n",
      "sadness 1.3472632184274025 town\n",
      "sadness 1.3241734119323534 terry\n",
      "-----------------------------------------\n",
      "Important words in surprise utterances\n",
      "surprise 1.7645522612382876 wow\n",
      "surprise 1.5387259637519688 julio\n",
      "surprise 1.4824048552369564 dress\n",
      "surprise 1.4710507302037734 dancer\n",
      "surprise 1.4494337808219053 wha\n",
      "surprise 1.3864573580639459 oww\n",
      "surprise 1.3542275712832261 forty\n",
      "surprise 1.2957350409959125 yike\n",
      "surprise 1.2914842440678984 fool\n",
      "surprise 1.2662487675389806 believe\n",
      "surprise 1.2626274133698232 grade\n",
      "surprise 1.2522220001195896 whoa\n",
      "surprise 1.2481912110809539 pull\n",
      "surprise 1.189768107441422 chip\n",
      "surprise 1.1804425874375055 size\n",
      "surprise 1.1161491004135178 contraction\n",
      "surprise 1.10982617247191 flip\n",
      "surprise 1.087807514975587 nurse\n",
      "surprise 1.0873045227506482 mark\n",
      "surprise 1.085007772597561 40\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def average_importances(model):\n",
    "    coef_avg = 0\n",
    "    for classifier in model.calibrated_classifiers_:\n",
    "        coef_avg = coef_avg + classifier.base_estimator.coef_\n",
    "        \n",
    "    coef_avg  = coef_avg/len(model.calibrated_classifiers_)\n",
    "    return coef_avg\n",
    "\n",
    "def f_importances(importances, names, n=20):\n",
    "    class_labels = label_encoder.classes_\n",
    "    \n",
    "    for num, imp in enumerate(importances):\n",
    "        emotion = class_labels[num]\n",
    "        topn = sorted(zip(imp,names), reverse=True)[:n]\n",
    "        \n",
    "        print(\"Important words in {} utterances\".format(emotion))\n",
    "        for coef, feat in topn:\n",
    "            print(emotion, coef, feat)\n",
    "        print(\"-----------------------------------------\")\n",
    "\n",
    "print('Most important features per emotion: 1A')\n",
    "feature_names = utterance_vec_1A.get_feature_names()\n",
    "importances = average_importances(svm_linear_clf_1A)\n",
    "f_importances(importances, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f8e622-4e4d-4f61-83eb-82c121f871dd",
   "metadata": {},
   "source": [
    "### Filter B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c676bfc-cd46-453c-a707-68a503f8ad53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determiner and pronouns {'some', 'that', 'all', 'they', 'he', 'an', 'i', 'yourself', 'those', 'every', 'its', 'either', 'ya', \"you're\", 'hers', 'both', \"my'this\", 'yours', 'your', \"i'm\", '’s', 'her', \"they're\", 'his', 'we', 'any', 'these', 'a', 'you', \"i'i\", 'each', 'my', 'no', 'our', 'himself', 'themselves', 'one', \"'em\", 'myself', 'mine', 'this', 'the', 'their', 'it', 'another', \"'s\", 'she'}\n",
      "Min_df 2\n"
     ]
    }
   ],
   "source": [
    "low_df_test_1B, DTandPRP_test_1B, clean_test_1B = \\\n",
    "remove_DT_PRP(2, test_data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31d05227-29b4-4f34-898a-6a01385559c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_count_1B = utterance_vec_1B.transform(clean_test_1B)\n",
    "test_tfidf_1B = tfidf_transformer.fit_transform(test_count_1B)\n",
    "\n",
    "y_pred_svm_1B = svm_linear_clf_1B.predict(test_tfidf_1B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fce4c065-36c1-47b2-87ce-3602e10dab31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger' 'disgust' 'fear' 'joy' 'sadness' 'surprise']\n",
      "BoW TFIDF SVM LINEAR: MELD, Filter B\n",
      "Word mininum document frequency 2 ; DT PRP removed\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.442688  0.324638  0.374582       345\n",
      "           1   1.000000  0.029412  0.057143        68\n",
      "           2   0.500000  0.060000  0.107143        50\n",
      "           3   0.443953  0.748756  0.557407       402\n",
      "           4   0.505495  0.221154  0.307692       208\n",
      "           5   0.546296  0.629893  0.585124       281\n",
      "\n",
      "    accuracy                       0.473412      1354\n",
      "   macro avg   0.573072  0.335642  0.331515      1354\n",
      "weighted avg   0.504319  0.473412  0.436463      1354\n",
      "\n",
      "Confusion matrix SVM, BoW MELD, Filter B\n",
      "['anger' 'disgust' 'fear' 'joy' 'sadness' 'surprise']\n",
      "[[112   0   0 163  11  59]\n",
      " [ 19   2   0  27   6  14]\n",
      " [ 14   0   3  18   5  10]\n",
      " [ 44   0   1 301  16  40]\n",
      " [ 39   0   2  97  46  24]\n",
      " [ 25   0   0  72   7 177]]\n"
     ]
    }
   ],
   "source": [
    "report1B = classification_report(test_classes,y_pred_svm_1B,digits = 6)\n",
    "print(label_encoder.classes_)\n",
    "print('BoW TFIDF SVM LINEAR: MELD, Filter B')\n",
    "print('Word mininum document frequency', min_df, \"; DT PRP removed\")\n",
    "print(report1B)\n",
    "\n",
    "print('Confusion matrix SVM, BoW MELD, Filter B')\n",
    "print(label_encoder.classes_)\n",
    "print(sklearn.metrics.confusion_matrix(test_classes,y_pred_svm_1B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4813af15-5dfb-4fc4-901a-e8227a49a07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probabilities_1B = svm_linear_clf_1B.predict_proba(test_tfidf_1B)\n",
    "\n",
    "pred_labels_1B = []\n",
    "for predicted_label in y_pred_svm_1B:\n",
    "    pred_labels_1B.append(label_encoder.classes_[predicted_label])\n",
    "\n",
    "gold_labels_1B = []\n",
    "for gold_label in test_classes:\n",
    "    gold_labels_1B.append(label_encoder.classes_[gold_label])\n",
    "\n",
    "result_frame1B = pd.DataFrame(pred_probabilities_1B*100, columns=label_encoder.classes_)\n",
    "\n",
    "result_frame1B['Chat']= list(meld_dftest['Utterance'])\n",
    "result_frame1B['Prediction']=pred_labels_1B\n",
    "result_frame1B['Gold']=gold_labels_1B\n",
    "\n",
    "result_frame1B.to_csv(\"result_frame1B.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "63f6364d-ae9c-4357-b210-1ab06ba85cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>Chat</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Gold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.914321</td>\n",
       "      <td>5.744996</td>\n",
       "      <td>8.733563</td>\n",
       "      <td>29.244748</td>\n",
       "      <td>7.023625</td>\n",
       "      <td>35.338748</td>\n",
       "      <td>Why do all you're coffee mugs have numbers on ...</td>\n",
       "      <td>surprise</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.148226</td>\n",
       "      <td>5.112113</td>\n",
       "      <td>5.458403</td>\n",
       "      <td>52.121119</td>\n",
       "      <td>14.897871</td>\n",
       "      <td>3.262269</td>\n",
       "      <td>Oh. That's so Monica can keep track. That way ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.664124</td>\n",
       "      <td>4.624781</td>\n",
       "      <td>0.784580</td>\n",
       "      <td>45.874474</td>\n",
       "      <td>2.186192</td>\n",
       "      <td>21.865850</td>\n",
       "      <td>Push!</td>\n",
       "      <td>joy</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.951878</td>\n",
       "      <td>6.663199</td>\n",
       "      <td>7.649186</td>\n",
       "      <td>13.439925</td>\n",
       "      <td>51.485383</td>\n",
       "      <td>2.810430</td>\n",
       "      <td>Push 'em out, push 'em out, harder, harder.</td>\n",
       "      <td>sadness</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31.840525</td>\n",
       "      <td>6.647776</td>\n",
       "      <td>7.327343</td>\n",
       "      <td>26.336274</td>\n",
       "      <td>11.931238</td>\n",
       "      <td>15.916844</td>\n",
       "      <td>Push 'em out, push 'em out, way out!</td>\n",
       "      <td>anger</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       anger   disgust      fear        joy    sadness   surprise  \\\n",
       "0  13.914321  5.744996  8.733563  29.244748   7.023625  35.338748   \n",
       "1  19.148226  5.112113  5.458403  52.121119  14.897871   3.262269   \n",
       "2  24.664124  4.624781  0.784580  45.874474   2.186192  21.865850   \n",
       "3  17.951878  6.663199  7.649186  13.439925  51.485383   2.810430   \n",
       "4  31.840525  6.647776  7.327343  26.336274  11.931238  15.916844   \n",
       "\n",
       "                                                Chat Prediction      Gold  \n",
       "0  Why do all you're coffee mugs have numbers on ...   surprise  surprise  \n",
       "1  Oh. That's so Monica can keep track. That way ...        joy     anger  \n",
       "2                                              Push!        joy       joy  \n",
       "3        Push 'em out, push 'em out, harder, harder.    sadness       joy  \n",
       "4               Push 'em out, push 'em out, way out!      anger       joy  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_frame1B.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6073a634-b02d-4327-87b7-9b1bd5583208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most important features per emotion: 1B\n",
      "Important words in anger utterances\n",
      "anger 1.693612461876011 tape\n",
      "anger 1.614785222225591 calm\n",
      "anger 1.5201460511540483 oboe\n",
      "anger 1.504128271368633 mad\n",
      "anger 1.4819404056686212 puck\n",
      "anger 1.462976671944109 garbage\n",
      "anger 1.445775564305268 table\n",
      "anger 1.4358343821895163 easy\n",
      "anger 1.4311620711579942 gimme\n",
      "anger 1.353733224177083 bloody\n",
      "anger 1.344203950395651 bitch\n",
      "anger 1.3426957676864837 quarter\n",
      "anger 1.3424136965059186 scrud\n",
      "anger 1.3278274654237006 knuckle\n",
      "anger 1.3131064638616585 choice\n",
      "anger 1.3020290384125368 york\n",
      "anger 1.3011097227434945 cramp\n",
      "anger 1.2867341345561323 jackass\n",
      "anger 1.2844905336447041 nothin'\n",
      "anger 1.2485350603651733 fancy\n",
      "-----------------------------------------\n",
      "Important words in disgust utterances\n",
      "disgust 1.853043201820205 ew\n",
      "disgust 1.771912102216519 disgusting\n",
      "disgust 1.7039667355156578 behave\n",
      "disgust 1.6953516943282883 violate\n",
      "disgust 1.6516117916952766 eww\n",
      "disgust 1.6268312307781705 boat\n",
      "disgust 1.5165538761449802 clinic\n",
      "disgust 1.4402137939566528 toilet\n",
      "disgust 1.4242368382806045 mindy\n",
      "disgust 1.3979621173275996 drug\n",
      "disgust 1.3480393334468854 pig\n",
      "disgust 1.3471992820616225 ugh\n",
      "disgust 1.316767921734104 ai\n",
      "disgust 1.2782682903171518 body\n",
      "disgust 1.2347117447405442 shame\n",
      "disgust 1.2281131616329741 hate\n",
      "disgust 1.1780530227715977 typical\n",
      "disgust 1.1080927999197132 smell\n",
      "disgust 1.1061195350832165 evil\n",
      "disgust 1.0934543723732435 hairy\n",
      "-----------------------------------------\n",
      "Important words in fear utterances\n",
      "fear 2.132577680338479 scared\n",
      "fear 1.8970224751491345 act\n",
      "fear 1.6033711729724345 judge\n",
      "fear 1.29930100354988 conscious\n",
      "fear 1.2859682344802534 swear\n",
      "fear 1.2170261865762764 twelve\n",
      "fear 1.1762019015731835 possibly\n",
      "fear 1.1683629712114922 brave\n",
      "fear 1.1345624420013536 purse\n",
      "fear 1.1164874299821477 stomach\n",
      "fear 1.093994065830793 uh\n",
      "fear 1.0837732326148233 rip\n",
      "fear 1.0788845294055958 treeger\n",
      "fear 1.0264372365572212 gun\n",
      "fear 1.0239937781804205 scary\n",
      "fear 1.0154938971139396 test\n",
      "fear 0.9764979133433893 sir\n",
      "fear 0.9666840127596693 key\n",
      "fear 0.9472809101724536 yet\n",
      "fear 0.9450658543202669 brochure\n",
      "-----------------------------------------\n",
      "Important words in joy utterances\n",
      "joy 1.8280247935252156 yay\n",
      "joy 1.7780296015337698 great\n",
      "joy 1.7547850974866226 glad\n",
      "joy 1.611565087184767 love\n",
      "joy 1.6000393468775855 alan\n",
      "joy 1.5576749465584439 amazing\n",
      "joy 1.465392422309168 incredible\n",
      "joy 1.4382346800873171 fun\n",
      "joy 1.4110806803703346 isabella\n",
      "joy 1.4061164774640407 decide\n",
      "joy 1.386079286470895 thank\n",
      "joy 1.3819636383097675 worker\n",
      "joy 1.37115436945416 sail\n",
      "joy 1.3478315857387417 tomorrow\n",
      "joy 1.344503950858117 barcelona\n",
      "joy 1.337470448127309 suit\n",
      "joy 1.3273596664498495 magic\n",
      "joy 1.2659293694607612 cool\n",
      "joy 1.259152967778499 beautiful\n",
      "joy 1.2534033522851795 sweet\n",
      "-----------------------------------------\n",
      "Important words in sadness utterances\n",
      "sadness 1.9846361069806016 sorry\n",
      "sadness 1.7999434096266047 band\n",
      "sadness 1.7108771603035475 club\n",
      "sadness 1.4885405867589028 exist\n",
      "sadness 1.4802813716044927 n’t\n",
      "sadness 1.4651729363293948 hurry\n",
      "sadness 1.4600990154619196 side\n",
      "sadness 1.4561075476614818 puppet\n",
      "sadness 1.4057545825297333 awful\n",
      "sadness 1.3890837225013977 hard\n",
      "sadness 1.377056611569337 story\n",
      "sadness 1.366613336266047 worry\n",
      "sadness 1.3604608814916201 terry\n",
      "sadness 1.3574593771323038 sad\n",
      "sadness 1.3488225951871557 lately\n",
      "sadness 1.3425009767418934 during\n",
      "sadness 1.330578598476119 chippy\n",
      "sadness 1.324487139600501 group\n",
      "sadness 1.3168603690322993 estelle\n",
      "sadness 1.2946338414722325 princess\n",
      "-----------------------------------------\n",
      "Important words in surprise utterances\n",
      "surprise 2.258030313824077 ?\n",
      "surprise 2.186945180247967 wow\n",
      "surprise 1.71157733696517 whoa\n",
      "surprise 1.6695838442085886 julio\n",
      "surprise 1.6110749101687847 oww\n",
      "surprise 1.6001953421777901 believe\n",
      "surprise 1.5212580387762376 dress\n",
      "surprise 1.491003960531224 dancer\n",
      "surprise 1.4523943550862441 my\n",
      "surprise 1.4278509301773674 yike\n",
      "surprise 1.4219986511770029 size\n",
      "surprise 1.4065577834358391 contraction\n",
      "surprise 1.4008309000418222 40\n",
      "surprise 1.3851043254100681 mark\n",
      "surprise 1.3451562891946893 wha\n",
      "surprise 1.3442036125682257 fool\n",
      "surprise 1.3199504879222903 dear\n",
      "surprise 1.242945938312257 hmm\n",
      "surprise 1.2409855292109495 word\n",
      "surprise 1.2191690831000095 misunderstand\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('Most important features per emotion: 1B')\n",
    "feature_names = utterance_vec_1B.get_feature_names()\n",
    "importances = average_importances(svm_linear_clf_1B)\n",
    "f_importances(importances, feature_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
